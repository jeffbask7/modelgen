{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.util import add_cyclic_point\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import cartopy.feature as cfeature\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.tri import Triangulation\n",
    "import shapely.speedups\n",
    "from shapely import Polygon\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "import xarray as xr\n",
    "import cfgrib\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.transform import from_bounds\n",
    "from rasterio.crs import CRS\n",
    "#import h3\n",
    "from pyproj import Transformer\n",
    "import json\n",
    "import pprint\n",
    "#import dask\n",
    "import eccodes\n",
    "import pygrib\n",
    "import psycopg2\n",
    "from osgeo import gdal\n",
    "from collections import defaultdict\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "@dataclass\n",
    "class DateTimeParts:\n",
    "    year: int\n",
    "    month: int\n",
    "    day: int\n",
    "    hour: int\n",
    "    minute: int\n",
    "\n",
    "    @classmethod\n",
    "    def from_datetime(cls, dt: datetime):\n",
    "        return cls(year=dt.year, month=dt.month, day=dt.day, hour=dt.hour, minute=dt.minute)\n",
    "    \n",
    "def windCalc(u,v):\n",
    "        #print('windCalc Function')\n",
    "        wind_abs = np.sqrt(u**2 + v**2)\n",
    "        wind_dir_trig_to = np.arctan2(u/wind_abs, v/wind_abs)\n",
    "        wind_dir_trig_to_degrees = wind_dir_trig_to * 180/np.pi ## -111.6 degrees\n",
    "        wind_dir = wind_dir_trig_to_degrees + 180\n",
    "        return wind_abs * 2.23694 #TO MPH\n",
    "def K_to_F(temp):\n",
    "    temp = ((temp - 273.15) * (9/5)) + 32\n",
    "    return temp\n",
    "\n",
    "def F_to_K(temp):\n",
    "    temp = ((temp - 32) * 5/9) + 273.15\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilterGrib2(runDate, indexHour, model='NBM'):\n",
    "    index_list = []\n",
    "    runTime = runDate.hour\n",
    "    year = runDate.year\n",
    "    month = runDate.month\n",
    "    day = runDate.day\n",
    "    fcsthr = indexHour\n",
    "    runTime_str = str(runTime).zfill(2)\n",
    "    fcsthr_str = str(fcsthr).zfill(3)\n",
    "    runDate = rf'{year}{str(month).zfill(2)}{str(day).zfill(2)}'\n",
    "    byte_ranges = defaultdict(list)\n",
    "\n",
    "    if model == 'NBM':\n",
    "        url_name = rf'https://noaa-nbm-grib2-pds.s3.amazonaws.com/blend.{runDate}/{runTime_str}/core/blend.t{runTime_str}z.core.f{fcsthr_str}.co.grib2'\n",
    "    else:\n",
    "        raise ValueError('Invalid model type')\n",
    "\n",
    "    index_url = f\"{url_name}.idx\"\n",
    "    print(f\"Index URL: {index_url}\")\n",
    "    \n",
    "    response = requests.get(index_url)\n",
    "    index_content = response.text.splitlines()\n",
    "\n",
    "   # Conditions for matching variables and levels dynamically\n",
    "    conditions = {\n",
    "        'ASNOW': lambda param, level: param == 'ASNOW' and ((len(level) < 30) and (level.split(\":\"))[-1] == ''),\n",
    "        'WIND': lambda param, level: param == 'WIND' and (((level.split(\":\"))[-1] == '') and (level.split(\":\"))[0][0] != 's'),\n",
    "        'TMP': lambda param, level: param == 'TMP' and '2 m above ground' in level,\n",
    "        'APCP': lambda param, level: param == 'APCP' and ((len(level) < 30) and (level.split(\":\"))[-1] == ''),\n",
    "        'DSWRF': lambda param, level: param == 'DSWRF' and 'surface' in level\n",
    "    }\n",
    "\n",
    "    conditions = {\n",
    "        'TMIN': lambda param, level: param == 'TMIN' and (level.split(\":\"))[-1] == '',\n",
    "        'TMAX': lambda param, level: param == 'TMAX' and (level.split(\":\"))[-1] == ''\n",
    "    }\n",
    "\n",
    "    prev_param = None  # Variable to track the previous parameter\n",
    "    prev_level = None  # Track the previous level for handling the end byte\n",
    "    get_next_startbyte = False\n",
    "\n",
    "    for line in index_content:\n",
    "        indexDict = line.split(\":\")\n",
    "        startByte = indexDict[1]\n",
    "        prev_startByte = startByte\n",
    "\n",
    "        #add current startbyte as endbyte for previous param\n",
    "        if get_next_startbyte:\n",
    "            pass\n",
    "            #print('next startbyte: ', prev_startByte)\n",
    "\n",
    "\n",
    "        param = indexDict[3].strip()  \n",
    "        #level = indexDict[4].strip() \n",
    "        level = \":\".join(indexDict[4:]).strip()  # Combine everything after index 3 to form the level\n",
    "\n",
    "        # Check if the current line matches any of the conditions\n",
    "        if get_next_startbyte and (len(byte_ranges) > 0):\n",
    "            #print('adding endbyte', prev_startByte, prev_param )\n",
    "            byte_ranges[prev_param][-1]['end'] = prev_startByte\n",
    "            #print(byte_ranges)\n",
    "            get_next_startbyte = False\n",
    "\n",
    "        \"\"\" if param in conditions:\n",
    "            condition = conditions[param]  # Get the condition for the param \"\"\"\n",
    "\n",
    "        for condition_name, condition_func in conditions.items():\n",
    "            if condition_func(param, level):\n",
    "                #if condition(param, level):  # Call the condition function with param and level\n",
    "                print(f'Matched {param} {level}: Start byte {startByte}')\n",
    "                #print('get_next_startbyte set to True')\n",
    "                get_next_startbyte = True\n",
    "\n",
    "                # If it's a new range, start tracking it\n",
    "                if prev_param != param or prev_level != level:\n",
    "                    byte_ranges[param].append({'start': startByte, 'end': None})\n",
    "\n",
    "                    # Update tracking variables\n",
    "                prev_param = condition_name\n",
    "                prev_level = level\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # Combine byte ranges as needed for each variable\n",
    "    index_list = []\n",
    "    for var, ranges in byte_ranges.items():\n",
    "        for byte_range in ranges:\n",
    "            start = byte_range['start']\n",
    "            end = byte_range['end']  # If end is None, use start as the end\n",
    "            #print(f\"Appending range for {var}: {start}-{end}\")\n",
    "            index_list.append(f\"{start}-{end}\")\n",
    "\n",
    "\n",
    "        # Download and merge GRIB file\n",
    "    #gribFile = f\"data/gribs/{model.lower()}/{model.lower()}-{runDate}_{runTime_str}_{fcsthr_str}.grb2\"\n",
    "    #gribFile = f\"data/gribs/{model.lower()}/latest/{model.lower()}-{fcsthr_str}.grb2\"\n",
    "    gribFile = f\"data/gribs/{model.lower()}/maxmin/{model.lower()}-{fcsthr_str}.grb2\"\n",
    "\n",
    "    if os.path.exists(gribFile):\n",
    "        os.remove(gribFile)\n",
    "\n",
    "    if len(index_list) > 0:\n",
    "        for byte_range in index_list:\n",
    "            print(f\"Downloading byte range: {byte_range}\")\n",
    "            command = rf'curl --range {byte_range} {url_name} >> {gribFile}'\n",
    "            os.system(command)\n",
    "        else:\n",
    "            print(f'no matches for forecast hour {fcsthr_str} ')\n",
    "\n",
    "    #return gribFile\n",
    "    return index_list, gribFile\n",
    "\n",
    "\"\"\" modelrun = datetime(2024,10,17,18,0,0)\n",
    "indexHour = 6\n",
    "model = 'nbm'\n",
    "datetime_parts = DateTimeParts.from_datetime(modelrun)\n",
    "print('getFilterGrib2')\n",
    "index_list, gribFile, condition_func = getFilterGrib2(datetime_parts, indexHour)\n",
    "print(index_list) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if condition_func('WIND', ':10 m above ground:6 hour fcst'):\n",
    "    print('yup')\n",
    "else:\n",
    "    print('nope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "modelrunTimes = []\n",
    "modelforecastTimes= []\n",
    "modelforecastSteps = []\n",
    "dir_root = ''\n",
    "for indexHour in range(12,241,6):\n",
    "    modelrun = datetime(2024,10,19,12,0,0)\n",
    "    model = 'NBM'\n",
    "    #modelrun2 = datetime(2021,2,4,23,30,0)\n",
    "    datetime_parts = DateTimeParts.from_datetime(modelrun)\n",
    "    print('getFilterGrib')\n",
    "    index_list, result = getFilterGrib2(datetime_parts, indexHour)\n",
    "    #grbs = open(result)\n",
    "    if os.path.exists(result):\n",
    "        result_list.append(result)\n",
    "        print('result', result)\n",
    "    #command2 = f'cp {result} {dir_root}data/gribs/{model.lower()}/latest/{model.lower()}-latest_{str(indexHour).zfill(3)}.grb2'\n",
    "    #subprocess.call(command2, shell=True)\n",
    "#modelvars = list(ds_temp.data_vars)\n",
    "str_list = ''\n",
    "for name in result_list:\n",
    "    if not str_list:\n",
    "        str_list = str_list + name\n",
    "    else:\n",
    "        str_list = str_list + ' ' + name\n",
    "    print(str_list)\n",
    "latestGrb = f'{dir_root}data/gribs/{model.lower()}/maxmin/{model.lower()}-latest.grb2'\n",
    "if os.path.exists(latestGrb):\n",
    "    os.remove(latestGrb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_list)\n",
    "latestGrb = f'data/gribs/{model.lower()}/latest/{model.lower()}-latest.grb2'\n",
    "latestGrb = f'data/gribs/{model.lower()}/maxmin/{model.lower()}-latest.grb2'\n",
    "command_cp = f'cp {result_list[1]} {latestGrb}'\n",
    "subprocess.call(command_cp, shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_list[1:])\n",
    "for grib_single in result_list:\n",
    "    print(grib_single)\n",
    "    command_append = f'wgrib2 -append {grib_single} -grib {latestGrb}'\n",
    "    subprocess.call(command_append, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'nbm'\n",
    "#gribfile = f'data/gribs/{model.lower()}/latest/{model.lower()}-024.grb2'\n",
    "latestGrb = f'data/gribs/{model.lower()}/latest/{model.lower()}-latest.grb2'\n",
    "latestGrb = f'data/gribs/{model.lower()}/maxmin/{model.lower()}-latest.grb2'\n",
    "gribfile = latestGrb\n",
    "grbs = pygrib.open(gribfile)\n",
    "for grb in grbs:\n",
    "    print(grb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grbs = pygrib.open(gribfile)\n",
    "tgrb_max = grbs.select(name = 'Maximum temperature')\n",
    "tgrb_min = grbs.select(name = 'Minimum temperature')\n",
    "tempgrbs_max = (tgrb_max)\n",
    "for  tmp in tempgrbs_max:\n",
    "    print(tmp)\n",
    "tempgrbs_min = (tgrb_min)\n",
    "for  tmp in tempgrbs_min:\n",
    "    print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tgrb = grbs.select(shortName = '2t')\n",
    "tgrb = grbs.select(name = 'Maximum temperature')\n",
    "#tgrb = grbs.select(paramId = 167) #t2 ens std\n",
    "tsd = (tgrb[3])\n",
    "tsd4 = (tgrb[4])\n",
    "print(tsd)\n",
    "print(tsd4)\n",
    "print(dir(tsd))\n",
    "print(tsd.validDate)\n",
    "tsd_list = list(dir(tsd))\n",
    "tsd_keys = (tsd.keys())\n",
    "print('2 metre temperature:K (instant):lambert:heightAboveGround:level 2 m:fcst time 36 hrs:from 202410151900:ens std dev')\n",
    "for key in tsd_list:\n",
    "    try:\n",
    "        print(key, tsd[key])\n",
    "    except:\n",
    "        print(key, ' does not exist')\n",
    "print('****************************************************')\n",
    "print('2 metre temperature:K (instant):lambert:heightAboveGround:level 2 m:fcst time 36 hrs:from 202410151900')\n",
    "tsd4_list = list(dir(tsd4))\n",
    "tsd4_keys = (tsd4.keys())\n",
    "for key4 in tsd4_list:\n",
    "    try:\n",
    "        print(key4, tsd4[key4])\n",
    "    except:\n",
    "        print(key4, ' does not exist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grb1 = grbs[1]\n",
    "print(grb1)\n",
    "lats, lons = grb1.latlons()\n",
    "# Flatten the lat/lon arrays to create a 2D list of points\n",
    "grid_points = np.column_stack((lats.ravel(), lons.ravel()))\n",
    "\n",
    "# Build a KDTree from the lat/lon grid\n",
    "tree = KDTree(grid_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_coords = pd.read_csv('stations.csv')\n",
    "coordinates = df_coords[['lat', 'lon']].values\n",
    "#coordinates = [(lat1, lon1), (lat2, lon2)]  # Replace with your actual coordinates\n",
    "\n",
    "# Find the nearest grid points for all coordinates\n",
    "nearest_indices = [tree.query([lat, lon])[1] for lat, lon in coordinates]\n",
    "#nearest_indices = [tree.query([lat, lon])[1] for lat, lon in coordinates]\n",
    "\n",
    "# Initialize an empty dictionary to hold the time series data\n",
    "#data = {coord: [] for coord in coordinates}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createOutput(var, nearest_indices, df_coords):   \n",
    "    data = {icao: [] for icao in df_coords['ICAO']}\n",
    "    grbs_all = pygrib.open(gribfile)\n",
    "    #tgrb = grbs_all.select(shortName = '2t')\n",
    "    tgrb = grbs.select(name=var)\n",
    "    tempgrbs = tgrb\n",
    "\n",
    "    # Loop through the GRIB messages to extract data for all variables and times\n",
    "    for grb in tempgrbs:\n",
    "        print(grb)\n",
    "        var_name = grb.name\n",
    "        valid_time = grb.validDate\n",
    "\n",
    "        # Flatten the data grid to align with the grid points\n",
    "        data_values = (K_to_F(grb.values.ravel()).round(0))\n",
    "\n",
    "        # For each row in the DataFrame, get the value from the nearest grid point\n",
    "        for (icao, idx) in zip(df_coords['ICAO'], nearest_indices):\n",
    "            nearest_value = data_values[idx]\n",
    "\n",
    "            # Collect the value, time, and variable for the given ICAO\n",
    "            data[icao].append({\n",
    "                'time': valid_time,\n",
    "                'variable': var_name,\n",
    "                'value': int(nearest_value),\n",
    "                'lat': df_coords.loc[df_coords['ICAO'] == icao, 'lat'].values[0],\n",
    "                'lon': df_coords.loc[df_coords['ICAO'] == icao, 'lon'].values[0],\n",
    "                'city': df_coords.loc[df_coords['ICAO'] == icao, 'CITY'].values[0],  # Add city\n",
    "                'state': df_coords.loc[df_coords['ICAO'] == icao, 'STATE'].values[0]  # Add state\n",
    "            })\n",
    "\n",
    "    # Create an empty list to store rows for the DataFrame\n",
    "    trows = []\n",
    "\n",
    "    # Flatten the data dictionary into the list of rows\n",
    "    for icao, records in data.items():\n",
    "        for record in records:\n",
    "            # Append each record as a dictionary to the rows list\n",
    "            trows.append({\n",
    "                'ICAO': icao,\n",
    "                'time': record['time'],\n",
    "                'variable': record['variable'],\n",
    "                'value': record['value'],\n",
    "                'lat': record['lat'],\n",
    "                'lon': record['lon'],\n",
    "                'CITY': record['city'],  # Add city to DataFrame\n",
    "                'STATE': record['state']  # Add state to DataFrame\n",
    "            })\n",
    "    print(trows)\n",
    "\n",
    "    # Convert the list of rows into a DataFrame\n",
    "    result_df = pd.DataFrame(trows)\n",
    "    result_df.to_csv(f'pointfcst_{model}.csv', index=False)\n",
    "\n",
    "    # The final DataFrame contains the time series for each ICAO code\n",
    "    print(result_df)\n",
    "\n",
    "    features = []\n",
    "    for icao, records in data.items():\n",
    "        # Extract lat/lon from one of the records (all will have the same lat/lon for the same ICAO)\n",
    "        lat = records[0]['lat']\n",
    "        lon = records[0]['lon']\n",
    "        city = records[0]['city']  # Extract city\n",
    "        state = records[0]['state']  # Extract state\n",
    "\n",
    "        # Prepare time series for this ICAO\n",
    "        time_series = [{\n",
    "            'time': str(record['time']),\n",
    "            'variable': str(record['variable']),\n",
    "            'value': str(record['value'])\n",
    "        } for record in records]\n",
    "\n",
    "        # Create the GeoJSON feature for this ICAO with the time series\n",
    "        feature = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"geometry\": {\n",
    "                \"type\": \"Point\",\n",
    "                \"coordinates\": [str(lon), str(lat)]\n",
    "            },\n",
    "            \"properties\": {\n",
    "                \"ICAO\": icao,\n",
    "                \"CITY\": city,  # Add city to GeoJSON\n",
    "                \"STATE\": state,  # Add state to GeoJSON\n",
    "                \"time_series\": time_series\n",
    "            }\n",
    "        }\n",
    "        features.append(feature)\n",
    "\n",
    "    # Create the final GeoJSON structure\n",
    "    geojson_data = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": features\n",
    "    }\n",
    "\n",
    "    # Determine file name based on the variable type\n",
    "    if var == 'Maximum temperature':\n",
    "        var_out = 'maxtemp'\n",
    "    elif var == 'Minimum temperature':\n",
    "        var_out = 'mintemp'\n",
    "    else:\n",
    "        var_out = ''\n",
    "    \n",
    "    # Export to GeoJSON file\n",
    "    with open(f'output_data_{var_out}.geojson', 'w') as f:\n",
    "        json.dump(geojson_data, f, indent=4)\n",
    "    \n",
    "    # Export to GeoJSON file\n",
    "    with open(f'/var/www/fapi/app/static/data/point/nbm/output_data_{var_out}.geojson', 'w') as f:\n",
    "        json.dump(geojson_data, f, indent=4)\n",
    "\n",
    "\n",
    "    print(\"GeoJSON with time series exported successfully!\")\n",
    "\n",
    "\n",
    "df_coords = pd.read_csv('stations.csv')\n",
    "coordinates = df_coords[['lat', 'lon']].values\n",
    "nearest_indices = [tree.query([lat, lon])[1] for lat, lon in coordinates]\n",
    "createOutput('Maximum temperature', nearest_indices, df_coords)\n",
    "createOutput('Minimum temperature', nearest_indices, df_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for icao, records in data.items():\n",
    "    # Extract lat/lon from one of the records (all will have the same lat/lon for the same ICAO)\n",
    "    lat = records[0]['lat']\n",
    "    lon = records[0]['lon']\n",
    "\n",
    "    # Prepare time series for this ICAO\n",
    "    time_series = [{\n",
    "        'time': str(record['time']),\n",
    "        'variable': str(record['variable']),\n",
    "        'value': str(record['value'])\n",
    "    } for record in records]\n",
    "\n",
    "    # Create the GeoJSON feature for this ICAO with the time series\n",
    "    feature = {\n",
    "        \"type\": \"Feature\",\n",
    "        \"geometry\": {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [str(lon), str(lat)]\n",
    "        },\n",
    "        \"properties\": {\n",
    "            \"ICAO\": icao,\n",
    "            \"time_series\": time_series\n",
    "        }\n",
    "    }\n",
    "    features.append(feature)\n",
    "\n",
    "# Create the final GeoJSON structure\n",
    "geojson_data = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": features\n",
    "}\n",
    "\n",
    "# Export to GeoJSON file\n",
    "with open('output_data5.geojson', 'w') as f:\n",
    "    json.dump(geojson_data, f, indent=4)\n",
    "\n",
    "print(\"GeoJSON with time series exported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('output_data.csv', index=False)\n",
    "\n",
    "# Create GeoJSON structure from the DataFrame\n",
    "features = []\n",
    "for i, row in result_df.iterrows():\n",
    "    feature = {\n",
    "        \"type\": \"Feature\",\n",
    "        \"geometry\": {\n",
    "            \"type\": \"Point\",\n",
    "            \"coordinates\": [row['lon'], row['lat']]\n",
    "        },\n",
    "        \"properties\": {\n",
    "            \"ICAO\": row['ICAO'],\n",
    "            \"time\": str(row['time']),\n",
    "            \"variable\": row['variable'],\n",
    "            \"value\": row['value']\n",
    "        }\n",
    "    }\n",
    "    features.append(feature)\n",
    "\n",
    "# Create the final GeoJSON structure\n",
    "geojson_data = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": features\n",
    "}\n",
    "\n",
    "# Export to GeoJSON file\n",
    "with open('output_data.geojson', 'w') as f:\n",
    "    json.dump(geojson_data, f, indent=4)\n",
    "\n",
    "print(\"Data exported successfully as CSV and GeoJSON!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_data.geojson', 'r') as f:\n",
    "    geojson_data = json.load(f)\n",
    "\n",
    "# Print to check structure\n",
    "#print(json.dumps(geojson_data, indent=4))\n",
    "print(geojson_data['features'])\n",
    "print(geojson_data['features'][-1]['properties'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('stations2.csv')\n",
    "\n",
    "# Print the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Define the custom order for the REGION column\n",
    "regions = ['SOUTH', 'CENTRAL', 'NORTH', 'WEST']\n",
    "\n",
    "# Convert the 'REGION' column to a categorical type with the specified order\n",
    "df['REGION'] = pd.Categorical(df['REGION'], categories=regions, ordered=True)\n",
    "\n",
    "# Sort the DataFrame by 'REGION' (custom order) and 'CITY' (alphabetically)\n",
    "df_sorted = df.sort_values(['REGION', 'CITY'])\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "print(\"Sorted DataFrame by REGION and CITY:\")\n",
    "print(df_sorted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
