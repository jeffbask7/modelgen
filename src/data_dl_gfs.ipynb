{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import requests\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "import xarray as xr\n",
    "import cfgrib\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.transform import from_bounds\n",
    "from rasterio.crs import CRS\n",
    "#import h3\n",
    "from pyproj import Transformer\n",
    "import json\n",
    "import pprint\n",
    "import dask\n",
    "import eccodes\n",
    "import pygrib\n",
    "import psycopg2\n",
    "from osgeo import gdal\n",
    "from collections import defaultdict\n",
    "from scipy.spatial import KDTree\n",
    "from datetime import datetime, timedelta\n",
    "import rioxarray\n",
    "from shapely.geometry import box\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.units import units\n",
    "from typing import Optional\n",
    "\n",
    "os.chdir('/home/jeff/modelgen')\n",
    "\n",
    "@dataclass\n",
    "class DateTimeParts:\n",
    "    year: int\n",
    "    month: int\n",
    "    day: int\n",
    "    hour: int\n",
    "    month_str: Optional[str] = None\n",
    "    day_str: Optional[str] = None\n",
    "    hour_str: Optional[str] = None\n",
    "    date_str: Optional[str] = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_datetime(cls, dt: datetime):\n",
    "        month_str=str(dt.month).zfill(2)\n",
    "        day_str = str(dt.day).zfill(2)\n",
    "        hour_str=str(dt.hour).zfill(2)\n",
    "        date_str = str(dt.year)+month_str+day_str\n",
    "        return cls(year=dt.year, month=dt.month, day=dt.day, hour=dt.hour, month_str=month_str, day_str=day_str, hour_str=hour_str, date_str=date_str)\n",
    "    \n",
    "def windCalc(u,v):\n",
    "        #print('windCalc Function')\n",
    "        wind_abs = np.sqrt(u**2 + v**2)\n",
    "        wind_dir_trig_to = np.arctan2(u/wind_abs, v/wind_abs)\n",
    "        wind_dir_trig_to_degrees = wind_dir_trig_to * 180/np.pi ## -111.6 degrees\n",
    "        wind_dir = wind_dir_trig_to_degrees + 180\n",
    "        return wind_abs * 2.23694 #TO MPH\n",
    "def K_to_F(temp):\n",
    "    temp = ((temp - 273.15) * (9/5)) + 32\n",
    "    return temp\n",
    "\n",
    "def F_to_K(temp):\n",
    "    temp = ((temp - 32) * 5/9) + 273.15\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInitTime_GFS():\n",
    "    mydate = (datetime.now())\n",
    "    print(mydate)\n",
    "    dateparts = DateTimeParts.from_datetime(mydate)\n",
    "\n",
    "    if dateparts.hour >= 2 and dateparts.hour <= 8:\n",
    "        hour_str = '00'\n",
    "    elif dateparts.hour >= 9 and dateparts.hour <= 15:\n",
    "        hour_str = '06'\n",
    "    elif dateparts.hour >= 16 and dateparts.hour <= 21:\n",
    "        hour_str = '12'\n",
    "    elif dateparts.hour >= 22:\n",
    "        hour_str = '18'\n",
    "    elif dateparts.hour < 2:\n",
    "        hour_str = '18'\n",
    "        dateparts = DateTimeParts.from_datetime(mydate - dt.timedelta(days=1))\n",
    "    else:\n",
    "        print(f'warning: forecast hour {dateparts.hour} is not between 0 and 23')\n",
    "        hour_str = None\n",
    "    print(dateparts, hour_str)\n",
    "    return dateparts, hour_str\n",
    "\n",
    "getInitTime_GFS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateparts, hour_str = getInitTime_GFS()\n",
    "modelrun = datetime(year=dateparts.year,month=dateparts.month,day=dateparts.day,hour=dateparts.hour)\n",
    "datetime_parts = DateTimeParts.from_datetime(modelrun)\n",
    "datetime_parts\n",
    "datetime_parts.date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilterGrib(runDate, indexHour, model='gfs'):\n",
    "    index_list = []\n",
    "    runTime = runDate.hour\n",
    "    year = runDate.year\n",
    "    month = runDate.month\n",
    "    day = runDate.day\n",
    "    fcsthr = indexHour\n",
    "    runTime_str = str(runTime).zfill(2)\n",
    "    fcsthr_str = str(fcsthr).zfill(3)\n",
    "    runDate = rf'{year}{str(month).zfill(2)}{str(day).zfill(2)}'\n",
    "    byte_ranges = defaultdict(list)\n",
    "\n",
    "    if model == 'nbm':\n",
    "        url_name = rf'https://noaa-nbm-grib2-pds.s3.amazonaws.com/blend.{runDate}/{runTime_str}/core/blend.t{runTime_str}z.core.f{fcsthr_str}.co.grib2'\n",
    "    elif model == 'gfs':\n",
    "        url_name = rf'https://noaa-gfs-bdp-pds.s3.amazonaws.com/gfs.{runDate}/{runTime_str}/atmos/gfs.t{runTime_str}z.pgrb2.0p25.f{fcsthr_str}'\n",
    "    elif model == 'gfs-graph':\n",
    "        url_name = rf'https://noaa-nws-graphcastgfs-pds.s3.amazonaws.com/graphcastgfs.{runDate}/{runTime_str}/forecasts_13_levels/graphcastgfs.t{runTime_str}z.pgrb2.0p25.f{fcsthr_str}'\n",
    "    else:\n",
    "        raise ValueError('Invalid model type')\n",
    "\n",
    "    index_url = f\"{url_name}.idx\"\n",
    "    print(f\"Index URL: {index_url}\")\n",
    "    \n",
    "    response = requests.get(index_url)\n",
    "    index_content = response.text.splitlines()\n",
    "\n",
    "# Conditions for matching variables and levels dynamically\n",
    "    conditions = {\n",
    "        #'ASNOW': lambda param, level: param == 'ASNOW' and ((len(level) < 30) and (level.split(\":\"))[-1] == ''),\n",
    "        #'WIND': lambda param, level: param == 'WIND' and (((level.split(\":\"))[-1] == '') and (level.split(\":\"))[0][0] != 's'),\n",
    "        'TMP': lambda param, level: param == 'TMP' and '2 m above ground' in level,\n",
    "        #'APCP': lambda param, level: param == 'APCP' and ((len(level) < 30) and (level.split(\":\"))[-1] == ''),\n",
    "        #'DSWRF': lambda param, level: param == 'DSWRF' and 'surface' in level,\n",
    "        #'TMIN': lambda param, level: param == 'TMIN' and (level.split(\":\"))[-1] == '',\n",
    "        #'TMAX': lambda param, level: param == 'TMAX' and (level.split(\":\"))[-1] == ''\n",
    "    }\n",
    "\n",
    "    \"\"\" conditions = {\n",
    "        'TMIN': lambda param, level: param == 'TMIN' and (level.split(\":\"))[-1] == '',\n",
    "        'TMAX': lambda param, level: param == 'TMAX' and (level.split(\":\"))[-1] == ''\n",
    "    } \"\"\"\n",
    "\n",
    "    prev_param = None  # Variable to track the previous parameter\n",
    "    prev_level = None  # Track the previous level for handling the end byte\n",
    "    get_next_startbyte = False\n",
    "\n",
    "    for line in index_content:\n",
    "        indexDict = line.split(\":\")\n",
    "        startByte = indexDict[1]\n",
    "        prev_startByte = startByte\n",
    "\n",
    "        #add current startbyte as endbyte for previous param\n",
    "        if get_next_startbyte:\n",
    "            pass\n",
    "            #print('next startbyte: ', prev_startByte)\n",
    "\n",
    "\n",
    "        param = indexDict[3].strip()  \n",
    "        #level = indexDict[4].strip() \n",
    "        level = \":\".join(indexDict[4:]).strip()  # Combine everything after index 3 to form the level\n",
    "\n",
    "        # Check if the current line matches any of the conditions\n",
    "        if get_next_startbyte and (len(byte_ranges) > 0):\n",
    "            #print('adding endbyte', prev_startByte, prev_param )\n",
    "            byte_ranges[prev_param][-1]['end'] = prev_startByte\n",
    "            #print(byte_ranges)\n",
    "            get_next_startbyte = False\n",
    "\n",
    "        \"\"\" if param in conditions:\n",
    "            condition = conditions[param]  # Get the condition for the param \"\"\"\n",
    "\n",
    "        for condition_name, condition_func in conditions.items():\n",
    "            if condition_func(param, level):\n",
    "                #if condition(param, level):  # Call the condition function with param and level\n",
    "                print(f'Matched {param} {level}: Start byte {startByte}')\n",
    "                #print('get_next_startbyte set to True')\n",
    "                get_next_startbyte = True\n",
    "\n",
    "                # If it's a new range, start tracking it\n",
    "                if prev_param != param or prev_level != level:\n",
    "                    byte_ranges[param].append({'start': startByte, 'end': None})\n",
    "\n",
    "                    # Update tracking variables\n",
    "                prev_param = condition_name\n",
    "                prev_level = level\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # Combine byte ranges as needed for each variable\n",
    "    index_list = []\n",
    "    for var, ranges in byte_ranges.items():\n",
    "        for byte_range in ranges:\n",
    "            start = byte_range['start']\n",
    "            end = byte_range['end']  # If end is None, use start as the end\n",
    "            #print(f\"Appending range for {var}: {start}-{end}\")\n",
    "            index_list.append(f\"{start}-{end}\")\n",
    "\n",
    "\n",
    "        # Download and merge GRIB file\n",
    "    #gribFile = f\"data/gribs/{model.lower()}/{model.lower()}-{runDate}_{runTime_str}_{fcsthr_str}.grb2\"\n",
    "    gribFile = f\"data/gribs/{model.lower()}/latest/{model.lower()}-{fcsthr_str}.grb2\"\n",
    "    #gribFile = f\"data/gribs/{model.lower()}/maxmin/{model.lower()}-{fcsthr_str}.grb2\"\n",
    "\n",
    "    if os.path.exists(gribFile):\n",
    "        os.remove(gribFile)\n",
    "\n",
    "    if len(index_list) > 0:\n",
    "        for byte_range in index_list:\n",
    "            print(f\"Downloading byte range: {byte_range}\")\n",
    "            command = rf'curl --range {byte_range} {url_name} >> {gribFile}'\n",
    "            os.system(command)\n",
    "        else:\n",
    "            print(f'no matches for forecast hour {fcsthr_str} ')\n",
    "\n",
    "    #return gribFile\n",
    "    return index_list, gribFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendGrib(model, result_list):\n",
    "    #model = 'ndfd'\n",
    "    latestGrb = f'data/gribs/{model.lower()}/latest/{model.lower()}-latest.grb2'\n",
    "    command_cp = f'cp {result_list[0]} {latestGrb}'\n",
    "    subprocess.call(command_cp, shell=True)\n",
    "\n",
    "    for grib_single in result_list[1:]:\n",
    "        print(grib_single)\n",
    "        command_append = f'wgrib2 -append {grib_single} -grib {latestGrb}'\n",
    "        subprocess.call(command_append, shell=True)\n",
    "    return pygrib.open(latestGrb)\n",
    "\n",
    "#grbs = appendGrib(model, result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "#os.chdir('/home/jeff/modelgen')\n",
    "#print(os.getcwd())\n",
    "dateparts, hour_str = getInitTime_GFS()\n",
    "print(dateparts)\n",
    "result_list = []\n",
    "modelrunTimes = []\n",
    "modelforecastTimes= []\n",
    "modelforecastSteps = []\n",
    "dir_root = ''\n",
    "model = 'gfs-graph'\n",
    "for indexHour in range(24,25,6):\n",
    "    #modelrun = datetime(dateparts.year,dateparts.month,dateparts.day,int(hour_str),0,0)\n",
    "    modelrun = datetime(dateparts.year,dateparts.month,dateparts.day,0,0,0)\n",
    "    datetime_parts = DateTimeParts.from_datetime(modelrun)\n",
    "    print(datetime_parts)\n",
    "    print('getFilterGrib')\n",
    "    index_list, result = getFilterGrib(datetime_parts, indexHour, model)\n",
    "    if os.path.exists(result):\n",
    "        result_list.append(result)\n",
    "        print('result', result)\n",
    "\n",
    "latestGrb = f'{dir_root}data/gribs/{model.lower()}/latest/{model.lower()}-latest_test.grb2'\n",
    "if os.path.exists(latestGrb):\n",
    "    os.remove(latestGrb)\n",
    "\n",
    "\n",
    "grbs = appendGrib(model, result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wgrib2 data/gribs/gfs/latest/gfs-012.grb2 -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = f\"data/gribs/{model}/latest/{model}-latest.grb2\"\n",
    "\n",
    "grbs = pygrib.open(testfile)\n",
    "print(dir(grbs[1]))\n",
    "for grb in grbs[12:13]:\n",
    "    print(grb)\n",
    "    stepType = grb.step\n",
    "    print(stepType)\n",
    "    fhour = grb.validDate\n",
    "    print(fhour)\n",
    "    tunits = grb.parameterUnits\n",
    "    print(tunits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_kwargs = {'stepType' : f'{stepType}'}\n",
    "grib_file = cfgrib.open_file(testfile, filter_by_keys={'stepType': 'max'})\n",
    "# Print all metadata keys for the first message\n",
    "print(dir(grib_file))\n",
    "print(grib_file.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='gfs-graph'\n",
    "testfile = f\"data/gribs/{model}/latest/{model}-latest.grb2\"\n",
    "grbs = pygrib.open(testfile)\n",
    "grb = grbs[2]\n",
    "step = grb.step\n",
    "print('step', step)\n",
    "filter_kwargs = {'step': step}\n",
    "ds = xr.open_dataset(\n",
    "        testfile,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"filter_by_keys\": filter_kwargs}\n",
    "        )\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = \"data/gribs/gfs/latest/gfs-latest.grb2\"\n",
    "\n",
    "grbs = pygrib.open(testfile)\n",
    "print(dir(grbs[1]))\n",
    "for grb in grbs[1:2]:\n",
    "    #print(grb)\n",
    "    print(grb.shortName, grb.typeOfLevel, grb.level)\n",
    "    stepType = grb.stepType\n",
    "    step = grb.step\n",
    "    fhour = grb.validDate\n",
    "    print('step', step)\n",
    "    filter_kwargs = {'stepType' : f'{stepType}', 'step': step}\n",
    "    ds = xr.open_dataset(\n",
    "        testfile,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"filter_by_keys\": filter_kwargs})\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = \"data/gribs/gfs-graph/latest/gfs-graph-latest.grb2\"\n",
    "\n",
    "grbs = pygrib.open(testfile)\n",
    "print(dir(grbs[1]))\n",
    "for grb in grbs[2:4]:\n",
    "    #print(grb)\n",
    "    print(grb.shortName, grb.typeOfLevel, grb.level)\n",
    "    stepType = grb.stepType\n",
    "    step = grb.step\n",
    "    fhour = grb.validDate\n",
    "    print('step', step)\n",
    "    filter_kwargs = {'stepType' : f'{stepType}', 'step': step}\n",
    "    ds = xr.open_dataset(\n",
    "        testfile,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"filter_by_keys\": filter_kwargs}\n",
    "        )\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = cfgrib.open_datasets(testfile)\n",
    "\n",
    "# Print out attributes for each dataset to see which filters can apply\n",
    "for ds in datasets:\n",
    "    print(ds)\n",
    "    print(ds.attrs)  # Top-level attributes\n",
    "    for var in ds.variables:\n",
    "        print(f\"Variable: {var}\")\n",
    "        print(ds[var].attrs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "color_values = {\n",
    "    -30: 'maroon',\n",
    "    -20: 'teal',\n",
    "    -10: 'lavender',\n",
    "    0: 'white',\n",
    "    10: 'fuchsia',\n",
    "    20: 'purple',\n",
    "    30: 'blue',\n",
    "    40: 'aqua',\n",
    "    50: 'green',\n",
    "    60: 'yellow',\n",
    "    70: 'orange',\n",
    "    80: 'red',\n",
    "    90: 'purple',\n",
    "    100: 'white',\n",
    "    110: 'pink',\n",
    "    130: 'maroon'\n",
    "}\n",
    "\n",
    "# Create the normalized bounds (0 to 1) for the colormap\n",
    "norm = mcolors.Normalize(vmin=min(color_values.keys()), vmax=max(color_values.keys()))\n",
    "\n",
    "# Create a LinearSegmentedColormap\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "    'colormap_tempF',\n",
    "    [(norm(value), color) for value, color in color_values.items()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmap_name='gist_ncar'\n",
    "cmap_name = 'colormap_tempF'\n",
    "testfile = \"data/gribs/gfs/latest/gfs-latest.grb2\"\n",
    "grbs = pygrib.open(testfile)\n",
    "for grb in grbs[8:9]:\n",
    "    shortname = grb.shortName\n",
    "    longname = grb.name\n",
    "    validDate = grb.validDate\n",
    "    analDate = grb.analDate\n",
    "    fcstHour = grb.forecastTime\n",
    "    level = grb.level\n",
    "    levtype = grb.levtype\n",
    "    typeOfLevel = grb.typeOfLevel\n",
    "    unit = grb.units\n",
    "    cfName = grb.cfVarName\n",
    "    stepType = grb.stepType\n",
    "    step = grb.step\n",
    "    p_units = grb.parameterUnits\n",
    "    print('shortname', shortname)\n",
    "    print('longname', longname)\n",
    "    print('stepType', stepType)\n",
    "    print('step', step)\n",
    "    #step = timedelta(hours=fcstHour)\n",
    "    filter_kwargs = {'stepType' : f'{stepType}', 'step': step}\n",
    "    #filter_kwargs = {'shortName' : f'{shortname}', 'step': step }\n",
    "    ds = xr.open_dataset(\n",
    "        testfile,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"filter_by_keys\": filter_kwargs})\n",
    "    ds=ds.sel(latitude = slice(57,20), longitude= slice(230,300))\n",
    "    #ds = ds.sel(step=step)\n",
    "    #print(ds)\n",
    "    variables = list(ds.data_vars)\n",
    "    variable = ds[variables[0]]\n",
    "    variable = variable.rio.write_crs(\"EPSG:4326\")\n",
    "    variable.attrs['units'] = p_units\n",
    "    variable = variable.metpy.quantify() \n",
    "    variable_F = variable.metpy.convert_units('degF')\n",
    "    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    #contour = variable_F.plot(ax=ax, transform=ccrs.PlateCarree(), vmin=-30, vmax=130, cmap=cmap)\n",
    "    #contour = ax.contourf(variable.longitude, variable.latitude, variable_F.values, transform=ccrs.PlateCarree(), levels = 32, vmin=-30, vmax=130, cmap=cmap)\n",
    "    contour = ax.pcolormesh(variable.longitude, variable.latitude, variable_F.values, transform=ccrs.PlateCarree(), vmin=-30, vmax=130, cmap=cmap)\n",
    "    ax.add_feature(cfeature.STATES)\n",
    "    ax.coastlines()\n",
    "    fig.colorbar(contour, shrink=1.0, orientation='horizontal', pad=0.03, aspect=60)\n",
    "    #colorbar = contour.colorbar\n",
    "    #colorbar.ax.set_position([0.78, 0.3, 0.03, 0.4])\n",
    "    ax.set_extent([-130, -60, 20, 50])\n",
    "    time_init = pd.Timestamp(ds.time.values).strftime('%D %H')\n",
    "    time_valid = pd.Timestamp(ds.valid_time.values).strftime('%D %H')\n",
    "    plt.title(f\"GFS {shortname} deg F {time_init}Z fcst hr {step} {time_valid}Z\")\n",
    "    plt.savefig(f\"data/images/gfs_plot/gfs_{shortname}_{str(step).zfill(3)}.png\", bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(variable_F.metpy))\n",
    "print(variable_F.metpy.unit_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longname_list = []\n",
    "shortname_list = []\n",
    "analDate_list = []\n",
    "validDate_list = []\n",
    "typeOfLevel_list = []\n",
    "fcstHour_list = []\n",
    "level_list = []\n",
    "levtype_list = []\n",
    "filename_list = []\n",
    "modelname_list = []\n",
    "unit_list = []\n",
    "bounding_box_wkt_list = []\n",
    "testfile = \"data/gribs/gfs/latest/gfs-latest.grb2\"\n",
    "model = 'gfs'\n",
    "grbs = pygrib.open(testfile)\n",
    "for grb in grbs:\n",
    "    shortname = grb.shortName\n",
    "    longname = grb.name\n",
    "    validDate = grb.validDate\n",
    "    analDate = grb.analDate\n",
    "    fcstHour = grb.forecastTime\n",
    "    level = grb.level\n",
    "    levtype = grb.levtype\n",
    "    typeOfLevel = grb.typeOfLevel\n",
    "    modelname = model.upper()\n",
    "    unit = grb.units\n",
    "    cfName = grb.cfVarName\n",
    "    stepType = grb.stepType\n",
    "    step = timedelta(hours=fcstHour)\n",
    "\n",
    "    print(\"shortname\", shortname)\n",
    "    print('fcstHour', fcstHour)\n",
    "\n",
    "\n",
    "    #filter_kwargs = {\"shortName\": f\"{shortname}\", \"typeOfLevel\": f\"{typeOfLevel}\", \"level\": level}\n",
    "    #filter_kwargs = {'shortName' : f'{shortname}', 'stepType' : f'{stepType}', 'forecastTime' : f'{fcstHour}'}\n",
    "    filter_kwargs = {'shortName' : f'{shortname}'}\n",
    "    ds = xr.open_dataset(\n",
    "        testfile,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"filter_by_keys\": filter_kwargs})\n",
    "    ds = ds.sel(step=step)\n",
    "    print(ds)\n",
    "    variables = list(ds.data_vars)\n",
    "    #print('xvars', variables)\n",
    "    variable = ds[variables[0]]\n",
    "    \n",
    "    # HRRR variable = variable.rio.write_crs(\"+proj=lcc +lat_1=38.5 +lat_2=38.5 +lat_0=38.5 +lon_0=-97.5 +x_0=0 +y_0=0 +datum=WGS84 +units=m +a=6371229 +b=6371229\")\n",
    "    variable = variable.rio.write_crs(\"EPSG:4326\")\n",
    "    variable_3857 = variable.rio.reproject(\"EPSG:3857\")\n",
    "    bounds = variable.rio.bounds()\n",
    "    bounding_box = box(bounds[0], bounds[1], bounds[2], bounds[3])\n",
    "    bounding_box_wkt = bounding_box.wkt\n",
    "\n",
    "\n",
    "    \"\"\" variable = variable.rio.reproject(\"EPSG:3857\")\n",
    "    # Get bounds after reprojection to EPSG:3857\n",
    "    bounds = variable.rio.bounds()\n",
    "    # Define EPSG:3857 valid bounds\n",
    "    min_mercator, max_mercator = -20037508.34, 20037508.34\n",
    "    # Clip the bounds to stay within EPSG:3857 valid range\n",
    "    clipped_bounds = (\n",
    "        min(max(bounds[0], min_mercator), max_mercator),  # minx\n",
    "        min(max(bounds[1], min_mercator), max_mercator),  # miny\n",
    "        min(max(bounds[2], min_mercator), max_mercator),  # maxx\n",
    "        min(max(bounds[3], min_mercator), max_mercator)   # maxy\n",
    "    )\n",
    "    # Convert clipped bounds to a Shapely polygon\n",
    "    bounding_box = box(clipped_bounds[0], clipped_bounds[1], clipped_bounds[2], clipped_bounds[3])\n",
    "    # Convert to WKT format for PostGIS\n",
    "    bounding_box_wkt = bounding_box.wkt \"\"\"\n",
    "\n",
    "\n",
    "    #write geotiff\n",
    "    print('writing to raster', shortname, fcstHour)\n",
    "    rootdir = 'data/images/gfs'\n",
    "    rootdir_plot = 'data/images/gfs_plot'\n",
    "    filename = f\"{model}_{shortname}_{levtype}_{typeOfLevel}_{level}_{str(fcstHour).zfill(3)}\"\n",
    "    variable_3857.rio.to_raster(f\"{rootdir}/{filename}.tiff\")\n",
    "\n",
    "\n",
    "    print('writing to png', shortname, fcstHour)\n",
    "    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    variable.plot(ax=ax, transform=ccrs.PlateCarree(), cmap=\"viridis\")  # Adjust colormap as needed\n",
    "    #print('adding coastlines')\n",
    "    #ax.coastlines()  # Add coastlines for geographical context\n",
    "    #ax.gridlines(draw_labels=True)  # Add gridlines and labels if needed\n",
    "\n",
    "    # Save as PNG\n",
    "    plt.savefig(f\"{rootdir_plot}/{filename}.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    #append lists\n",
    "    longname_list.append(longname)\n",
    "    shortname_list.append(shortname)\n",
    "    analDate_list.append(analDate)\n",
    "    validDate_list.append(validDate)\n",
    "    fcstHour_list.append(fcstHour)\n",
    "    typeOfLevel_list.append(typeOfLevel)\n",
    "    level_list.append(level)\n",
    "    levtype_list.append(levtype)\n",
    "    filename_list.append(filename)\n",
    "    modelname_list.append(modelname)\n",
    "    unit_list.append(unit)\n",
    "    bounding_box_wkt_list.append(bounding_box_wkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectDB(model='gfs', varname='wind'):\n",
    "    #model = 'gfs'\n",
    "    # Database connection details\n",
    "    DB_HOST = 'localhost'\n",
    "    DB_PORT = '5432'\n",
    "    DB_NAME = 'geoserver_db'\n",
    "    DB_USER = 'geoserver_user'\n",
    "    DB_PASS = 'geoserver'\n",
    "\n",
    "    # Path to the directory containing TIFF files\n",
    "    TIFF_DIR = f'/usr/share/geoserver/data_dir/data/{model}_{varname}/'\n",
    "\n",
    "    # Connect to the PostGIS database\n",
    "    conn = psycopg2.connect(\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT,\n",
    "        dbname=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASS\n",
    "    ) \n",
    "\n",
    "    return conn\n",
    "\n",
    "conn = connectDB()\n",
    "print(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2.extras import execute_values\n",
    "\n",
    "conn = connectDB()\n",
    "\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "\n",
    "#preprocessed_geom_list = [f\"ST_GeomFromText('{wkt}', 3857)\" for wkt in bounding_box_wkt_list]\n",
    "\n",
    "# Create insertrows without including ST_GeomFromText in the values\n",
    "insertrows = [\n",
    "    (\n",
    "        modelname, shortname, longname, validDate, analDate, fcstHour,\n",
    "        level, levtype, typeOfLevel, unit, location, ingestion, the_geom\n",
    "    )\n",
    "    for modelname, shortname, longname, validDate, analDate, fcstHour,\n",
    "        level, levtype, typeOfLevel, unit, location, ingestion, the_geom\n",
    "    in zip(\n",
    "        modelname_list, shortname_list, longname_list, validDate_list, analDate_list,\n",
    "        fcstHour_list, level_list, levtype_list, typeOfLevel_list, unit_list,\n",
    "        filename_list, validDate_list, bounding_box_wkt_list  # Pass WKT directly here\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define the SQL insert query with ON CONFLICT for upsert\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO modeldata.meta_master (\n",
    "        modelname, shortname, longname, validDate, analDate, fcstHour,\n",
    "        level, levtype, typeOfLevel, unit, location, ingestion, the_geom\n",
    "    ) VALUES %s\n",
    "    ON CONFLICT (location) DO UPDATE SET\n",
    "        modelname = EXCLUDED.modelname,\n",
    "        shortname = EXCLUDED.shortname,\n",
    "        longname = EXCLUDED.longname,\n",
    "        validDate = EXCLUDED.validDate,\n",
    "        analDate = EXCLUDED.analDate,\n",
    "        fcstHour = EXCLUDED.fcstHour,\n",
    "        level = EXCLUDED.level,\n",
    "        levtype = EXCLUDED.levtype,\n",
    "        typeOfLevel = EXCLUDED.typeOfLevel,\n",
    "        unit = EXCLUDED.unit,\n",
    "        ingestion = EXCLUDED.ingestion,\n",
    "        the_geom = EXCLUDED.the_geom\n",
    "\"\"\"\n",
    "\n",
    "# Execute the bulk insert with execute_values, applying ST_GeomFromText in SQL\n",
    "from psycopg2.extras import execute_values\n",
    "execute_values(\n",
    "    cur,\n",
    "    insert_query,\n",
    "    insertrows,\n",
    "    template=\"(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, ST_GeomFromText(%s, 3857))\",\n",
    "    page_size=1000\n",
    ")\n",
    "\n",
    "# Commit the transaction\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in filename_list:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = 'data/gribs/gfs-graph/maxmin/gfs-graph-012.grb2'\n",
    "grbs = pygrib.open(testfile)\n",
    "grbs = grbs.select(name = '2 metre temperature')\n",
    "for grb in grbs:\n",
    "    print(grb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
