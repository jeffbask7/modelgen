{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import requests\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "import xarray as xr\n",
    "import cfgrib\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.transform import from_bounds\n",
    "from rasterio.crs import CRS\n",
    "#import h3\n",
    "from pyproj import Transformer\n",
    "import json\n",
    "import pprint\n",
    "import dask\n",
    "import eccodes\n",
    "import pygrib\n",
    "import psycopg2\n",
    "from osgeo import gdal\n",
    "from collections import defaultdict\n",
    "from scipy.spatial import KDTree\n",
    "from datetime import datetime, timedelta\n",
    "import rioxarray\n",
    "from shapely.geometry import box\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.units import units\n",
    "from typing import Optional\n",
    "\n",
    "os.chdir('/home/jeff/modelgen')\n",
    "\n",
    "@dataclass\n",
    "class DateTimeParts:\n",
    "    year: int\n",
    "    month: int\n",
    "    day: int\n",
    "    hour: int\n",
    "    month_str: Optional[str] = None\n",
    "    day_str: Optional[str] = None\n",
    "    hour_str: Optional[str] = None\n",
    "    date_str: Optional[str] = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_datetime(cls, dt: datetime):\n",
    "        month_str=str(dt.month).zfill(2)\n",
    "        day_str = str(dt.day).zfill(2)\n",
    "        hour_str=str(dt.hour).zfill(2)\n",
    "        date_str = str(dt.year)+month_str+day_str\n",
    "        return cls(year=dt.year, month=dt.month, day=dt.day, hour=dt.hour, month_str=month_str, day_str=day_str, hour_str=hour_str, date_str=date_str)\n",
    "    \n",
    "def windCalc(u,v):\n",
    "        #print('windCalc Function')\n",
    "        wind_abs = np.sqrt(u**2 + v**2)\n",
    "        wind_dir_trig_to = np.arctan2(u/wind_abs, v/wind_abs)\n",
    "        wind_dir_trig_to_degrees = wind_dir_trig_to * 180/np.pi ## -111.6 degrees\n",
    "        wind_dir = wind_dir_trig_to_degrees + 180\n",
    "        return wind_abs * 2.23694 #TO MPH\n",
    "def K_to_F(temp):\n",
    "    temp = ((temp - 273.15) * (9/5)) + 32\n",
    "    return temp\n",
    "\n",
    "def F_to_K(temp):\n",
    "    temp = ((temp - 32) * 5/9) + 273.15\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInitTime_GFS():\n",
    "    mydate = (datetime.now())\n",
    "    print(mydate)\n",
    "    dateparts = DateTimeParts.from_datetime(mydate)\n",
    "\n",
    "    if dateparts.hour >= 2 and dateparts.hour <= 8:\n",
    "        hour_str = '00'\n",
    "    elif dateparts.hour >= 9 and dateparts.hour <= 15:\n",
    "        hour_str = '06'\n",
    "    elif dateparts.hour >= 16 and dateparts.hour <= 21:\n",
    "        hour_str = '12'\n",
    "    elif dateparts.hour >= 22:\n",
    "        hour_str = '18'\n",
    "    elif dateparts.hour < 2:\n",
    "        hour_str = '18'\n",
    "        dateparts = DateTimeParts.from_datetime(mydate - dt.timedelta(days=1))\n",
    "    else:\n",
    "        print(f'warning: forecast hour {dateparts.hour} is not between 0 and 23')\n",
    "        hour_str = None\n",
    "    print(dateparts, hour_str)\n",
    "    return dateparts, hour_str\n",
    "\n",
    "getInitTime_GFS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateparts, hour_str = getInitTime_GFS()\n",
    "modelrun = datetime(year=dateparts.year,month=dateparts.month,day=dateparts.day,hour=dateparts.hour)\n",
    "datetime_parts = DateTimeParts.from_datetime(modelrun)\n",
    "datetime_parts\n",
    "datetime_parts.date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilterGrib(runDate, indexHour, model='gfs-test'):\n",
    "    index_list = []\n",
    "    runTime = runDate.hour\n",
    "    year = runDate.year\n",
    "    month = runDate.month\n",
    "    day = runDate.day\n",
    "    fcsthr = indexHour\n",
    "    runTime_str = str(runTime).zfill(2)\n",
    "    fcsthr_str = str(fcsthr).zfill(3)\n",
    "    runDate = rf'{year}{str(month).zfill(2)}{str(day).zfill(2)}'\n",
    "    byte_ranges = defaultdict(list)\n",
    "\n",
    "    if model == 'nbm':\n",
    "        url_name = rf'https://noaa-nbm-grib2-pds.s3.amazonaws.com/blend.{runDate}/{runTime_str}/core/blend.t{runTime_str}z.core.f{fcsthr_str}.co.grib2'\n",
    "    elif model == 'gfs-test':\n",
    "        url_name = rf'https://noaa-gfs-bdp-pds.s3.amazonaws.com/gfs.{runDate}/{runTime_str}/atmos/gfs.t{runTime_str}z.pgrb2.0p25.f{fcsthr_str}'\n",
    "    elif model == 'gfs-graph':\n",
    "        url_name = rf'https://noaa-nws-graphcastgfs-pds.s3.amazonaws.com/graphcastgfs.{runDate}/{runTime_str}/forecasts_13_levels/graphcastgfs.t{runTime_str}z.pgrb2.0p25.f{fcsthr_str}'\n",
    "    else:\n",
    "        raise ValueError('Invalid model type')\n",
    "\n",
    "    index_url = f\"{url_name}.idx\"\n",
    "    print(f\"Index URL: {index_url}\")\n",
    "    \n",
    "    response = requests.get(index_url)\n",
    "    index_content = response.text.splitlines()\n",
    "\n",
    "# Corrected conditions dictionary with unique keys\n",
    "    conditions = {\n",
    "        'PRMSL': lambda param, level: param == 'PRMSL',\n",
    "        'REFC': lambda param, level: param == 'REFC',\n",
    "        'CSNOW': lambda param, level: param == 'CSNOW',\n",
    "        'CFRZR': lambda param, level: param == 'CFRZR',\n",
    "        'CICEP': lambda param, level: param == 'CICEP',\n",
    "        'CFRZR': lambda param, level: param == 'CFRZR',\n",
    "        'HGT_500mb': lambda param, level: param == 'HGT' and '500 mb' in level,\n",
    "        'HGT_1000mb': lambda param, level: param == 'HGT' and '1000 mb' in level,\n",
    "        'TMP_2m_above_ground': lambda param, level: param == 'TMP' and '2 m above ground' in level,\n",
    "        'UGRD_10m_above_ground': lambda param, level: param == 'UGRD' and '10 m above ground' in level,\n",
    "        'VGRD_10m_above_ground': lambda param, level: param == 'VGRD' and '10 m above ground' in level,\n",
    "        'UGRD_100m_above_ground': lambda param, level: param == 'UGRD' and '100 m above ground' in level,\n",
    "        'VGRD_100m_above_ground': lambda param, level: param == 'VGRD' and '100 m above ground' in level,\n",
    "        'APCP': lambda param, level: param == 'APCP' and ((len(level) < 30) and (level.split(\":\"))[-1] == ''),\n",
    "    }\n",
    "\n",
    "    index_length = len(index_content)\n",
    "\n",
    "    # Process each line in the index file\n",
    "    for i in range(index_length):\n",
    "        line = index_content[i]\n",
    "        indexDict = line.split(\":\")\n",
    "        startByte = indexDict[1]\n",
    "        param = indexDict[3].strip()\n",
    "        level = \":\".join(indexDict[4:]).strip()\n",
    "\n",
    "        # Check if the current line matches any condition\n",
    "        matched = False\n",
    "        for condition_name, condition_func in conditions.items():\n",
    "            if condition_func(param, level):\n",
    "                matched = True\n",
    "                # Determine the end byte\n",
    "                if i < index_length - 1:\n",
    "                    next_line = index_content[i + 1]\n",
    "                    next_indexDict = next_line.split(\":\")\n",
    "                    endByte = next_indexDict[1]\n",
    "                    byte_length = int(endByte) - int(startByte)\n",
    "                else:\n",
    "                    # Last line; we don't have a next start byte\n",
    "                    byte_length = 0  # Or handle accordingly if you know the total file size\n",
    "\n",
    "                # Append the byte range to the index list\n",
    "                #index_list.append((int(startByte), int(endByte)))\n",
    "                index_list.append(f'{startByte}-{endByte}') #USE FOR CURL\n",
    "                #index_list.append((int(startByte), byte_length)) #USE FOR EARTKKIT.DATA\n",
    "                print(f\"Matched {param} {level}: Start byte {startByte}, Length {byte_length}\")\n",
    "                break  # No need to check other conditions once matched\n",
    "\n",
    "    gribFile = f\"data/gribs/{model.lower()}/latest/{model.lower()}-{fcsthr_str}.grb2\"\n",
    "    #gribFile = f\"data/gribs/{model.lower()}/maxmin/{model.lower()}-{fcsthr_str}.grb2\"\n",
    "\n",
    "    if os.path.exists(gribFile):\n",
    "        os.remove(gribFile)\n",
    "\n",
    "    if len(index_list) > 0:\n",
    "        for byte_range in index_list:\n",
    "            print(f\"Downloading byte range: {byte_range}\")\n",
    "            command = rf'curl --range {byte_range} {url_name} >> {gribFile}'\n",
    "            os.system(command)\n",
    "        else:\n",
    "            print(f'no matches for forecast hour {fcsthr_str} ')\n",
    "\n",
    "    #return gribFile\n",
    "    return index_list, gribFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendGrib(model, result_list):\n",
    "    #model = 'ndfd'\n",
    "    latestGrb = f'data/gribs/{model.lower()}/latest/{model.lower()}-latest.grb2'\n",
    "    command_cp = f'cp {result_list[0]} {latestGrb}'\n",
    "    subprocess.call(command_cp, shell=True)\n",
    "\n",
    "    for grib_single in result_list[1:]:\n",
    "        print(grib_single)\n",
    "        command_append = f'wgrib2 -append {grib_single} -grib {latestGrb}'\n",
    "        subprocess.call(command_append, shell=True)\n",
    "    return pygrib.open(latestGrb)\n",
    "\n",
    "#grbs = appendGrib(model, result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "#os.chdir('/home/jeff/modelgen')\n",
    "#print(os.getcwd())\n",
    "dateparts, hour_str = getInitTime_GFS()\n",
    "print(dateparts)\n",
    "result_list = []\n",
    "modelrunTimes = []\n",
    "modelforecastTimes= []\n",
    "modelforecastSteps = []\n",
    "dir_root = ''\n",
    "model = 'gfs-test'\n",
    "for indexHour in range(30,31,6):\n",
    "    #modelrun = datetime(dateparts.year,dateparts.month,dateparts.day,int(hour_str),0,0)\n",
    "    modelrun = datetime(dateparts.year,dateparts.month,dateparts.day,12,0,0)\n",
    "    datetime_parts = DateTimeParts.from_datetime(modelrun)\n",
    "    print(datetime_parts)\n",
    "    print('getFilterGrib')\n",
    "    index_list, result = getFilterGrib(datetime_parts, indexHour, model)\n",
    "    if os.path.exists(result):\n",
    "        result_list.append(result)\n",
    "        print('result', result)\n",
    "\n",
    "latestGrb = f'{dir_root}data/gribs/{model.lower()}/latest/{model.lower()}-latest_test.grb2'\n",
    "if os.path.exists(latestGrb):\n",
    "    os.remove(latestGrb)\n",
    "\n",
    "\n",
    "grbs = appendGrib(model, result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='gfs-test'\n",
    "testfile = f'data/gribs/{model.lower()}/latest/{model.lower()}-latest.grb2'\n",
    "grbs = pygrib.open(testfile)\n",
    "#grbs = grbs.select(name = '2 metre temperature')\n",
    "for grb in grbs:\n",
    "    print(grb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p6 = grbs[6]\n",
    "pt = grbs[7]\n",
    "print(p6)\n",
    "print(pt)\n",
    "\n",
    "print(pt.keys())\n",
    "print(p6.timeIncrement)\n",
    "print(pt.timeIncrement)\n",
    "print(p6.lengthOfTimeRange)\n",
    "print(pt.lengthOfTimeRange)\n",
    "print(pt.stepType)\n",
    "print(p6.stepRange)\n",
    "print(pt.stepRange)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step=24\n",
    "stepRange = '18-24'\n",
    "stepType = 'accum' #instant avg accum\n",
    "#shortname = [['prmsl', 'refc']]\n",
    "filter_kwargs = {'stepType' : f'{stepType}', 'stepRange': stepRange}\n",
    "#filter_kwargs = {'stepType' : f'{stepType}', 'lengthOfTimeRange': 24}\n",
    "#filter_kwargs = {'shortName' : f'{shortname}', 'step': step }\n",
    "#filter_kwargs = {'step': step }\n",
    "ds = xr.open_dataset(\n",
    "    testfile,\n",
    "    engine=\"cfgrib\",\n",
    "    backend_kwargs={\"filter_by_keys\": filter_kwargs})\n",
    "ds=ds.sel(latitude = slice(57,20), longitude= slice(230,300))\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(ds['tp'].values))\n",
    "print(ds['tp'].values.view())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refc_snow = ds['refc'].where(ds['csnow'] == 1, other=0) + 100\n",
    "refc_ice = ds['refc'].where((ds['cfrzr'] == 1) | (ds['cicep'] == 1), other=0) + 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(refc_ice.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "#ax.contourf(ds['refc'].longitude, ds['refc'].latitude, refc_ice, transform=ccrs.PlateCarree(), levels=levels_ice, vmin=210, vmax=280, norm=norm_ice, cmap=cmap_ice, extend='neither')\n",
    "contour3 = ax.contourf(ds['refc'].longitude, ds['refc'].latitude, ds['cfrzr'], transform=ccrs.PlateCarree())\n",
    "ax.add_feature(cfeature.STATES)\n",
    "ax.coastlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "#fig, ax = plt.subplots(figsize=(10, 8))\n",
    "#contour = variable_F.plot(ax=ax, transform=ccrs.PlateCarree(), vmin=-30, vmax=130, cmap=cmap)\n",
    "#contour = ax.contourf(variable.longitude, variable.latitude, variable_F.values, transform=ccrs.PlateCarree(), levels = 32, vmin=-30, vmax=130, cmap=cmap)\n",
    "#contour = ax.contour(ds['prmsl'].longitude, ds['prmsl'].latitude, ds['prmsl'].values)\n",
    "contour2 = ax.contourf(ds['refc'].longitude, ds['refc'].latitude, ds['refc'].values, transform=ccrs.PlateCarree(), levels=levels, vmin=10, vmax=80, norm=norm, cmap=cmap, extend='neither')\n",
    "contour3 = ax.contourf(ds['refc'].longitude, ds['refc'].latitude, refc_snow, transform=ccrs.PlateCarree(), levels=levels_snow, vmin=110, vmax=180, norm=norm_snow, cmap=cmap_snow, extend='neither')\n",
    "contour4 = ax.contourf(ds['refc'].longitude, ds['refc'].latitude, refc_ice, transform=ccrs.PlateCarree(), levels=levels_ice, vmin=210, vmax=280, norm=norm_ice, cmap=cmap_ice, extend='neither')\n",
    "contour = ax.contour(ds['prmsl'].longitude, ds['prmsl'].latitude, ds['prmsl'].values, transform=ccrs.PlateCarree(), colors='k', levels=plevels, linewidths=0.5)\n",
    "ax.add_feature(cfeature.STATES)\n",
    "ax.coastlines()\n",
    "fig.colorbar(contour2, shrink=1.0, orientation='horizontal', pad=0.03, aspect=60, extend='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = f\"data/gribs/{model}/latest/{model}-latest.grb2\"\n",
    "\n",
    "grbs = pygrib.open(testfile)\n",
    "print(dir(grbs[1]))\n",
    "for grb in grbs[12:13]:\n",
    "    print(grb)\n",
    "    stepType = grb.step\n",
    "    print(stepType)\n",
    "    fhour = grb.validDate\n",
    "    print(fhour)\n",
    "    tunits = grb.parameterUnits\n",
    "    print(tunits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_kwargs = {'stepType' : f'{stepType}'}\n",
    "grib_file = cfgrib.open_file(testfile, filter_by_keys={'stepType': 'max'})\n",
    "# Print all metadata keys for the first message\n",
    "print(dir(grib_file))\n",
    "print(grib_file.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='gfs-graph'\n",
    "testfile = f\"data/gribs/{model}/latest/{model}-latest.grb2\"\n",
    "grbs = pygrib.open(testfile)\n",
    "grb = grbs[2]\n",
    "step = grb.step\n",
    "print('step', step)\n",
    "filter_kwargs = {'step': step}\n",
    "ds = xr.open_dataset(\n",
    "        testfile,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"filter_by_keys\": filter_kwargs}\n",
    "        )\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = \"data/gribs/gfs/latest/gfs-latest.grb2\"\n",
    "\n",
    "grbs = pygrib.open(testfile)\n",
    "print(dir(grbs[1]))\n",
    "for grb in grbs[1:2]:\n",
    "    #print(grb)\n",
    "    print(grb.shortName, grb.typeOfLevel, grb.level)\n",
    "    stepType = grb.stepType\n",
    "    step = grb.step\n",
    "    fhour = grb.validDate\n",
    "    print('step', step)\n",
    "    filter_kwargs = {'stepType' : f'{stepType}', 'step': step}\n",
    "    ds = xr.open_dataset(\n",
    "        testfile,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"filter_by_keys\": filter_kwargs})\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = \"data/gribs/gfs-graph/latest/gfs-graph-latest.grb2\"\n",
    "\n",
    "grbs = pygrib.open(testfile)\n",
    "print(dir(grbs[1]))\n",
    "for grb in grbs[2:4]:\n",
    "    #print(grb)\n",
    "    print(grb.shortName, grb.typeOfLevel, grb.level)\n",
    "    stepType = grb.stepType\n",
    "    step = grb.step\n",
    "    fhour = grb.validDate\n",
    "    print('step', step)\n",
    "    filter_kwargs = {'stepType' : f'{stepType}', 'step': step}\n",
    "    ds = xr.open_dataset(\n",
    "        testfile,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"filter_by_keys\": filter_kwargs}\n",
    "        )\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = cfgrib.open_datasets(testfile)\n",
    "\n",
    "# Print out attributes for each dataset to see which filters can apply\n",
    "for ds in datasets:\n",
    "    print(ds)\n",
    "    print(ds.attrs)  # Top-level attributes\n",
    "    for var in ds.variables:\n",
    "        print(f\"Variable: {var}\")\n",
    "        print(ds[var].attrs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "color_values = {\n",
    "    -30: 'maroon',\n",
    "    -20: 'teal',\n",
    "    -10: 'lavender',\n",
    "    0: 'white',\n",
    "    10: 'fuchsia',\n",
    "    20: 'purple',\n",
    "    30: 'blue',\n",
    "    40: 'aqua',\n",
    "    50: 'green',\n",
    "    60: 'yellow',\n",
    "    70: 'orange',\n",
    "    80: 'red',\n",
    "    90: 'purple',\n",
    "    100: 'white',\n",
    "    110: 'pink',\n",
    "    130: 'maroon'\n",
    "}\n",
    "\n",
    "# Create the normalized bounds (0 to 1) for the colormap\n",
    "norm1 = mcolors.Normalize(vmin=min(color_values.keys()), vmax=max(color_values.keys()))\n",
    "\n",
    "# Create a LinearSegmentedColormap\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "    'colormap_tempF',\n",
    "    [(norm(value), color) for value, color in color_values.items()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "color_values_refc = {\n",
    "    5: '#FFFFFF00',\n",
    "    10: '#00FF0088',\n",
    "    20: 'green',\n",
    "    30: 'yellow',\n",
    "    40: 'orange',\n",
    "    50: 'red',\n",
    "    60: 'purple',\n",
    "    70: 'pink',\n",
    "    80: 'orange'\n",
    "}\n",
    "\n",
    "# Create the normalized bounds (0 to 1) for the colormap\n",
    "\"\"\" norm = mcolors.Normalize(vmin=min(color_values_refc.keys()), vmax=max(color_values_refc.keys()))\n",
    "# Create a LinearSegmentedColormap\n",
    "cmap_refc = mcolors.LinearSegmentedColormap.from_list(\n",
    "    'colormap_refc',\n",
    "    [(norm(value), color) for value, color in color_values_refc.items()]\n",
    ") \"\"\"\n",
    "\n",
    "norm = mcolors.BoundaryNorm(\n",
    "    boundaries=list(color_values_refc.keys()),\n",
    "    ncolors=len(color_values_refc),\n",
    "    extend='neither' )\n",
    "\n",
    "values = list(color_values_refc.values())\n",
    "levels = list(color_values_refc.keys())\n",
    "print(levels)\n",
    "print(values)\n",
    "cmap = mcolors.ListedColormap(values)\n",
    "\n",
    "norm = mcolors.BoundaryNorm(levels, ncolors=cmap.N, extend='neither')\n",
    "\n",
    "color_values_refc_snow = {\n",
    "    105: '#FFFFFF00',\n",
    "    110: '#0000FF88',\n",
    "    120: 'aqua',\n",
    "    130: 'blue',\n",
    "    140: 'purple',\n",
    "    150: 'pink',\n",
    "    160: 'white',\n",
    "    170: 'green',\n",
    "    180: 'orange'\n",
    "}\n",
    "\n",
    "# Create the normalized bounds (0 to 1) for the colormap\n",
    "\"\"\" norm = mcolors.Normalize(vmin=min(color_values_refc.keys()), vmax=max(color_values_refc.keys()))\n",
    "# Create a LinearSegmentedColormap\n",
    "cmap_refc = mcolors.LinearSegmentedColormap.from_list(\n",
    "    'colormap_refc',\n",
    "    [(norm(value), color) for value, color in color_values_refc.items()]\n",
    ") \"\"\"\n",
    "\n",
    "norm_snow = mcolors.BoundaryNorm(\n",
    "    boundaries=list(color_values_refc_snow.keys()),\n",
    "    ncolors=len(color_values_refc_snow),\n",
    "    extend='neither' )\n",
    "\n",
    "values_snow = list(color_values_refc_snow.values())\n",
    "levels_snow = list(color_values_refc_snow.keys())\n",
    "print(levels_snow)\n",
    "print(values_snow)\n",
    "cmap_snow = mcolors.ListedColormap(values_snow)\n",
    "\n",
    "norm_snow = mcolors.BoundaryNorm(levels_snow, ncolors=cmap_snow.N, extend='neither')\n",
    "\n",
    "color_values_refc_ice = {\n",
    "    205: '#FFFFFF00',\n",
    "    210: '#FF000088',\n",
    "    220: 'red',\n",
    "    230: 'yellow',\n",
    "    240: 'green',\n",
    "    250: 'brown',\n",
    "    260: 'white',\n",
    "    270: 'blue',\n",
    "    280: 'black'\n",
    "}\n",
    "\n",
    "# Create the normalized bounds (0 to 1) for the colormap\n",
    "\"\"\" norm = mcolors.Normalize(vmin=min(color_values_refc.keys()), vmax=max(color_values_refc.keys()))\n",
    "# Create a LinearSegmentedColormap\n",
    "cmap_refc = mcolors.LinearSegmentedColormap.from_list(\n",
    "    'colormap_refc',\n",
    "    [(norm(value), color) for value, color in color_values_refc.items()]\n",
    ") \"\"\"\n",
    "\n",
    "norm_ice = mcolors.BoundaryNorm(\n",
    "    boundaries=list(color_values_refc_ice.keys()),\n",
    "    ncolors=len(color_values_refc_ice),\n",
    "    extend='neither' )\n",
    "\n",
    "values_ice = list(color_values_refc_ice.values())\n",
    "levels_ice = list(color_values_refc_ice.keys())\n",
    "print(levels_ice)\n",
    "print(values_ice)\n",
    "cmap_ice = mcolors.ListedColormap(values_ice)\n",
    "\n",
    "norm_ice = mcolors.BoundaryNorm(levels_ice, ncolors=cmap_ice.N, extend='neither')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmap_name='gist_ncar'\n",
    "cmap_name = 'colormap_tempF'\n",
    "testfile = \"data/gribs/gfs/latest/gfs-latest.grb2\"\n",
    "grbs = pygrib.open(testfile)\n",
    "for grb in grbs[8:9]:\n",
    "    shortname = grb.shortName\n",
    "    longname = grb.name\n",
    "    validDate = grb.validDate\n",
    "    analDate = grb.analDate\n",
    "    fcstHour = grb.forecastTime\n",
    "    level = grb.level\n",
    "    levtype = grb.levtype\n",
    "    typeOfLevel = grb.typeOfLevel\n",
    "    unit = grb.units\n",
    "    cfName = grb.cfVarName\n",
    "    stepType = grb.stepType\n",
    "    step = grb.step\n",
    "    p_units = grb.parameterUnits\n",
    "    print('shortname', shortname)\n",
    "    print('longname', longname)\n",
    "    print('stepType', stepType)\n",
    "    print('step', step)\n",
    "    #step = timedelta(hours=fcstHour)\n",
    "    filter_kwargs = {'stepType' : f'{stepType}', 'step': step}\n",
    "    #filter_kwargs = {'shortName' : f'{shortname}', 'step': step }\n",
    "    ds = xr.open_dataset(\n",
    "        testfile,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"filter_by_keys\": filter_kwargs})\n",
    "    ds=ds.sel(latitude = slice(57,20), longitude= slice(230,300))\n",
    "    #ds = ds.sel(step=step)\n",
    "    #print(ds)\n",
    "    variables = list(ds.data_vars)\n",
    "    variable = ds[variables[0]]\n",
    "    variable = variable.rio.write_crs(\"EPSG:4326\")\n",
    "    variable.attrs['units'] = p_units\n",
    "    variable = variable.metpy.quantify() \n",
    "    variable_F = variable.metpy.convert_units('degF')\n",
    "    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    #contour = variable_F.plot(ax=ax, transform=ccrs.PlateCarree(), vmin=-30, vmax=130, cmap=cmap)\n",
    "    #contour = ax.contourf(variable.longitude, variable.latitude, variable_F.values, transform=ccrs.PlateCarree(), levels = 32, vmin=-30, vmax=130, cmap=cmap)\n",
    "    contour = ax.pcolormesh(variable.longitude, variable.latitude, variable_F.values, transform=ccrs.PlateCarree(), vmin=-30, vmax=130, cmap=cmap)\n",
    "    ax.add_feature(cfeature.STATES)\n",
    "    ax.coastlines()\n",
    "    fig.colorbar(contour, shrink=1.0, orientation='horizontal', pad=0.03, aspect=60)\n",
    "    #colorbar = contour.colorbar\n",
    "    #colorbar.ax.set_position([0.78, 0.3, 0.03, 0.4])\n",
    "    ax.set_extent([-130, -60, 20, 50])\n",
    "    time_init = pd.Timestamp(ds.time.values).strftime('%D %H')\n",
    "    time_valid = pd.Timestamp(ds.valid_time.values).strftime('%D %H')\n",
    "    plt.title(f\"GFS {shortname} deg F {time_init}Z fcst hr {step} {time_valid}Z\")\n",
    "    plt.savefig(f\"data/images/gfs_plot/gfs_{shortname}_{str(step).zfill(3)}.png\", bbox_inches='tight', pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(variable_F.metpy))\n",
    "print(variable_F.metpy.unit_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longname_list = []\n",
    "shortname_list = []\n",
    "analDate_list = []\n",
    "validDate_list = []\n",
    "typeOfLevel_list = []\n",
    "fcstHour_list = []\n",
    "level_list = []\n",
    "levtype_list = []\n",
    "filename_list = []\n",
    "modelname_list = []\n",
    "unit_list = []\n",
    "bounding_box_wkt_list = []\n",
    "testfile = \"data/gribs/gfs/latest/gfs-latest.grb2\"\n",
    "model = 'gfs'\n",
    "grbs = pygrib.open(testfile)\n",
    "for grb in grbs:\n",
    "    shortname = grb.shortName\n",
    "    longname = grb.name\n",
    "    validDate = grb.validDate\n",
    "    analDate = grb.analDate\n",
    "    fcstHour = grb.forecastTime\n",
    "    level = grb.level\n",
    "    levtype = grb.levtype\n",
    "    typeOfLevel = grb.typeOfLevel\n",
    "    modelname = model.upper()\n",
    "    unit = grb.units\n",
    "    cfName = grb.cfVarName\n",
    "    stepType = grb.stepType\n",
    "    step = timedelta(hours=fcstHour)\n",
    "\n",
    "    print(\"shortname\", shortname)\n",
    "    print('fcstHour', fcstHour)\n",
    "\n",
    "\n",
    "    #filter_kwargs = {\"shortName\": f\"{shortname}\", \"typeOfLevel\": f\"{typeOfLevel}\", \"level\": level}\n",
    "    #filter_kwargs = {'shortName' : f'{shortname}', 'stepType' : f'{stepType}', 'forecastTime' : f'{fcstHour}'}\n",
    "    filter_kwargs = {'shortName' : f'{shortname}'}\n",
    "    ds = xr.open_dataset(\n",
    "        testfile,\n",
    "        engine=\"cfgrib\",\n",
    "        backend_kwargs={\"filter_by_keys\": filter_kwargs})\n",
    "    ds = ds.sel(step=step)\n",
    "    print(ds)\n",
    "    variables = list(ds.data_vars)\n",
    "    #print('xvars', variables)\n",
    "    variable = ds[variables[0]]\n",
    "    \n",
    "    # HRRR variable = variable.rio.write_crs(\"+proj=lcc +lat_1=38.5 +lat_2=38.5 +lat_0=38.5 +lon_0=-97.5 +x_0=0 +y_0=0 +datum=WGS84 +units=m +a=6371229 +b=6371229\")\n",
    "    variable = variable.rio.write_crs(\"EPSG:4326\")\n",
    "    variable_3857 = variable.rio.reproject(\"EPSG:3857\")\n",
    "    bounds = variable.rio.bounds()\n",
    "    bounding_box = box(bounds[0], bounds[1], bounds[2], bounds[3])\n",
    "    bounding_box_wkt = bounding_box.wkt\n",
    "\n",
    "\n",
    "    \"\"\" variable = variable.rio.reproject(\"EPSG:3857\")\n",
    "    # Get bounds after reprojection to EPSG:3857\n",
    "    bounds = variable.rio.bounds()\n",
    "    # Define EPSG:3857 valid bounds\n",
    "    min_mercator, max_mercator = -20037508.34, 20037508.34\n",
    "    # Clip the bounds to stay within EPSG:3857 valid range\n",
    "    clipped_bounds = (\n",
    "        min(max(bounds[0], min_mercator), max_mercator),  # minx\n",
    "        min(max(bounds[1], min_mercator), max_mercator),  # miny\n",
    "        min(max(bounds[2], min_mercator), max_mercator),  # maxx\n",
    "        min(max(bounds[3], min_mercator), max_mercator)   # maxy\n",
    "    )\n",
    "    # Convert clipped bounds to a Shapely polygon\n",
    "    bounding_box = box(clipped_bounds[0], clipped_bounds[1], clipped_bounds[2], clipped_bounds[3])\n",
    "    # Convert to WKT format for PostGIS\n",
    "    bounding_box_wkt = bounding_box.wkt \"\"\"\n",
    "\n",
    "\n",
    "    #write geotiff\n",
    "    print('writing to raster', shortname, fcstHour)\n",
    "    rootdir = 'data/images/gfs'\n",
    "    rootdir_plot = 'data/images/gfs_plot'\n",
    "    filename = f\"{model}_{shortname}_{levtype}_{typeOfLevel}_{level}_{str(fcstHour).zfill(3)}\"\n",
    "    variable_3857.rio.to_raster(f\"{rootdir}/{filename}.tiff\")\n",
    "\n",
    "\n",
    "    print('writing to png', shortname, fcstHour)\n",
    "    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    variable.plot(ax=ax, transform=ccrs.PlateCarree(), cmap=\"viridis\")  # Adjust colormap as needed\n",
    "    #print('adding coastlines')\n",
    "    #ax.coastlines()  # Add coastlines for geographical context\n",
    "    #ax.gridlines(draw_labels=True)  # Add gridlines and labels if needed\n",
    "\n",
    "    # Save as PNG\n",
    "    plt.savefig(f\"{rootdir_plot}/{filename}.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    #append lists\n",
    "    longname_list.append(longname)\n",
    "    shortname_list.append(shortname)\n",
    "    analDate_list.append(analDate)\n",
    "    validDate_list.append(validDate)\n",
    "    fcstHour_list.append(fcstHour)\n",
    "    typeOfLevel_list.append(typeOfLevel)\n",
    "    level_list.append(level)\n",
    "    levtype_list.append(levtype)\n",
    "    filename_list.append(filename)\n",
    "    modelname_list.append(modelname)\n",
    "    unit_list.append(unit)\n",
    "    bounding_box_wkt_list.append(bounding_box_wkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectDB(model='gfs', varname='wind'):\n",
    "    #model = 'gfs'\n",
    "    # Database connection details\n",
    "    DB_HOST = 'localhost'\n",
    "    DB_PORT = '5432'\n",
    "    DB_NAME = 'geoserver_db'\n",
    "    DB_USER = 'geoserver_user'\n",
    "    DB_PASS = 'geoserver'\n",
    "\n",
    "    # Path to the directory containing TIFF files\n",
    "    TIFF_DIR = f'/usr/share/geoserver/data_dir/data/{model}_{varname}/'\n",
    "\n",
    "    # Connect to the PostGIS database\n",
    "    conn = psycopg2.connect(\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT,\n",
    "        dbname=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASS\n",
    "    ) \n",
    "\n",
    "    return conn\n",
    "\n",
    "conn = connectDB()\n",
    "print(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2.extras import execute_values\n",
    "\n",
    "conn = connectDB()\n",
    "\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "\n",
    "#preprocessed_geom_list = [f\"ST_GeomFromText('{wkt}', 3857)\" for wkt in bounding_box_wkt_list]\n",
    "\n",
    "# Create insertrows without including ST_GeomFromText in the values\n",
    "insertrows = [\n",
    "    (\n",
    "        modelname, shortname, longname, validDate, analDate, fcstHour,\n",
    "        level, levtype, typeOfLevel, unit, location, ingestion, the_geom\n",
    "    )\n",
    "    for modelname, shortname, longname, validDate, analDate, fcstHour,\n",
    "        level, levtype, typeOfLevel, unit, location, ingestion, the_geom\n",
    "    in zip(\n",
    "        modelname_list, shortname_list, longname_list, validDate_list, analDate_list,\n",
    "        fcstHour_list, level_list, levtype_list, typeOfLevel_list, unit_list,\n",
    "        filename_list, validDate_list, bounding_box_wkt_list  # Pass WKT directly here\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define the SQL insert query with ON CONFLICT for upsert\n",
    "insert_query = \"\"\"\n",
    "    INSERT INTO modeldata.meta_master (\n",
    "        modelname, shortname, longname, validDate, analDate, fcstHour,\n",
    "        level, levtype, typeOfLevel, unit, location, ingestion, the_geom\n",
    "    ) VALUES %s\n",
    "    ON CONFLICT (location) DO UPDATE SET\n",
    "        modelname = EXCLUDED.modelname,\n",
    "        shortname = EXCLUDED.shortname,\n",
    "        longname = EXCLUDED.longname,\n",
    "        validDate = EXCLUDED.validDate,\n",
    "        analDate = EXCLUDED.analDate,\n",
    "        fcstHour = EXCLUDED.fcstHour,\n",
    "        level = EXCLUDED.level,\n",
    "        levtype = EXCLUDED.levtype,\n",
    "        typeOfLevel = EXCLUDED.typeOfLevel,\n",
    "        unit = EXCLUDED.unit,\n",
    "        ingestion = EXCLUDED.ingestion,\n",
    "        the_geom = EXCLUDED.the_geom\n",
    "\"\"\"\n",
    "\n",
    "# Execute the bulk insert with execute_values, applying ST_GeomFromText in SQL\n",
    "from psycopg2.extras import execute_values\n",
    "execute_values(\n",
    "    cur,\n",
    "    insert_query,\n",
    "    insertrows,\n",
    "    template=\"(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, ST_GeomFromText(%s, 3857))\",\n",
    "    page_size=1000\n",
    ")\n",
    "\n",
    "# Commit the transaction\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plevels = np.linspace(88000,108000,51)\n",
    "print(plevels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(880,1080,4))\n",
    "print(len(x))\n",
    "for y in x:\n",
    "    print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
